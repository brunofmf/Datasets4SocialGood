{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Personality_Processes_GradientBoostedTrees.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMslfs4sC19U8zBiO5vQWRy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/brunofmf/Datasets4SocialGood/blob/master/Personality_Processes_GradientBoostedTrees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMjKFTdpevh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Wed Mar 18 16:21:35 2020\n",
        "\n",
        "@author: brunofmf\n",
        "\"\"\"\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import KFold, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, precision_score, recall_score\n",
        "from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "import joblib\n",
        "import time\n",
        "import json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ixhjbZPvzNla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### Random seeds definition ########################\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "np.random.seed(91190530)\n",
        "#np.random.seed(95191227)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_VOxJKfee2b5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### File System Interaction ########################\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "def enable_save_to_drive():\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "        \n",
        "'''\n",
        "Read datasets from file system or from google drive\n",
        "Return with data augmentation if with_da is true\n",
        "'''\n",
        "def read_dataset(with_da, colab):\n",
        "    if colab:\n",
        "        #give permission to save to drive\n",
        "        enable_save_to_drive()\n",
        "        #load dataset\n",
        "        from google.colab import files\n",
        "        import io\n",
        "        uploaded = files.upload()\n",
        "        for fn in uploaded.keys():\n",
        "            print('Uploaded file \"{name}\" with length {length} bytes'.format(name=fn, length=len(uploaded[fn])))\n",
        "        return pd.read_csv(io.BytesIO(uploaded[fn]))\n",
        "    else:\n",
        "        file_name = ('prepared_datasets/personality_final_WithDa_20200321.csv' if with_da else 'prepared_datasets/personality_final_NoDa_20200321.csv')\n",
        "        return pd.read_csv(file_name)\n",
        "\n",
        "'''\n",
        "Save file to file system or to google drive\n",
        "'''\n",
        "def save_file(name, results, trial, iteration, architecture, with_da, colab, testing=False):\n",
        "    if testing:\n",
        "        filename = name + '_Architecture' + str(architecture) + '_' + with_da + '_' + time.strftime(\"%Y%m%d%H%M\") + \".txt\"\n",
        "    else:\n",
        "        filename = name + '_Architecture' + str(architecture) + '_' + with_da + '_Trial' + str(trial) + '_Iteration' + str(iteration) + '_' + time.strftime(\"%Y%m%d%H%M\") + \".txt\"\n",
        "    if colab:\n",
        "        filepath = F'/content/gdrive/My Drive/Experiments/' + filename\n",
        "    else:\n",
        "        filepath = F'Experiments/' + filename\n",
        "    with open(filepath, 'w') as f:\n",
        "        f.write(json.dumps(results))\n",
        "        \n",
        "'''\n",
        "Save the best estimator found using nested-CV\n",
        "'''\n",
        "def save_best_estimator(best_estimator, architecture, with_da, colab):\n",
        "    filename = 'BestModel_Architecture' + str(architecture) + '_' + with_da + '_' + time.strftime(\"%Y%m%d%H%M\") + \".pkl\"\n",
        "    if colab:\n",
        "        filepath = F'/content/gdrive/My Drive/Experiments/' + filename\n",
        "    else:\n",
        "        filepath = F'Experiments/' + filename\n",
        "    joblib.dump(best_estimator, filepath)\n",
        "      \n",
        "'''\n",
        "Used to save CSVs (in particular, to save features' importance)\n",
        "'''\n",
        "def save_csv(df, architecture, with_da, colab, save_index=False):\n",
        "    filename = 'FeaturesImportance_Architecture' + str(architecture) + '_' + with_da + '_' + time.strftime(\"%Y%m%d%H%M\") + \".csv\"\n",
        "    if colab:\n",
        "        filepath = F'/content/gdrive/My Drive/Experiments/' + filename\n",
        "    else:\n",
        "        filepath = F'Experiments/' + filename\n",
        "    df.to_csv(filepath, index=save_index)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkO5h5kGfU_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### Data Definition ################################\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "'''\n",
        "Returns a tuning dictionary (hyperparameter search space)\n",
        "'''\n",
        "def tuning_dictionary():\n",
        "    return {\n",
        "        'estimator__n_estimators': [400, 500, 600],\n",
        "        'estimator__max_depth': [4, 8, 12],\n",
        "        'estimator__eta': [0.02, 0.05, 0.1], #aka learning rate\n",
        "        'estimator__gamma': [0.02, 0.04, 0.08],\n",
        "        'estimator__min_child_weight': [4, 6, 8],\n",
        "        'estimator__colsample_bytree': [0.3, 0.4]\n",
        "    }\n",
        "\n",
        "'''\n",
        "Split dataframe into X and y, depending on the architecture\n",
        "Returns (X, y)\n",
        "'''\n",
        "def split_x_y(df, architecture, testing=False):\n",
        "    df_aux = df.copy()\n",
        "    #if arch is 1 (regression) we want trait values and we drop binned traits. If arch is 2 (classifiers), it is the opposite\n",
        "    if architecture == 1:\n",
        "        df_aux.drop(df_aux.loc[:, 'extraversion_binned':'openess_binned'].columns, axis=1, inplace=True)\n",
        "    else:\n",
        "        df_aux.drop(df_aux.loc[:, 'extraversion_recalculated':'openess_recalculated'].columns, axis=1, inplace=True)\n",
        "    #which columns make our X (the one-hot encoded adjectives)\n",
        "    cols_X = pd.Series([('recalculated' not in col and 'binned' not in col) for col in df_aux.columns])\n",
        "    #split into X and y\n",
        "    #if testing, we will test the best estimator on the last 50 adjectives (not used to evaluate the model)\n",
        "    if testing:\n",
        "        X = df_aux.iloc[-50:, cols_X.values]\n",
        "        y = df_aux.iloc[-50:, ~cols_X.values]\n",
        "    else:\n",
        "        X = df_aux.loc[:, cols_X.values]\n",
        "        y = df_aux.loc[:, ~cols_X.values]\n",
        "    return X, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JqHEWjoOzh6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### Modelling and Fit ##############################\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "'''\n",
        "Dynamically build the gradient boosted estimator. If arch is 1 then XGBRegressor else XGBClassifier\n",
        "Returns a multi-output regressor/classifier which fits one regressor/classifier per label (we have 5 labels - the five traits)\n",
        "'''\n",
        "def build_model(architecture):\n",
        "    if architecture == 1:\n",
        "        estimator = XGBRegressor(\n",
        "            booster = 'gbtree', \n",
        "            objective = 'reg:squarederror',\n",
        "            eval_metric = 'rmse',\n",
        "            #tree_method='gpu_hist',\n",
        "            verbose=1)\n",
        "        multi_estimator = MultiOutputRegressor(estimator)\n",
        "    else:\n",
        "        estimator = XGBClassifier(\n",
        "            booster = 'gbtree',\n",
        "            objective = 'multi:softmax',\n",
        "            num_class = 3,\n",
        "            eval_metric = 'auc', #auc not gpu supported (https://xgboost.readthedocs.io/en/latest/gpu/index.html)\n",
        "            #tree_method='gpu_hist',\n",
        "            verbose=1)\n",
        "        multi_estimator = MultiOutputClassifier(estimator)\n",
        "    return multi_estimator\n",
        "\n",
        "'''\n",
        "Fitting the multi-output regressor/classifier\n",
        "Performs num_trials trials using nested-cross validation with outer k as outer_k_folds and inner k as inner_k_folds\n",
        "Returns the best_metric, best_estimator\n",
        "'''\n",
        "def find_best_model(model, X, y, param_grid, num_trials=2, outer_k_folds=2, inner_k_folds=3, num_iter=200, scoring='neg_root_mean_squared_error', architecture=1, with_da='no_da'):\n",
        "    #find the best model of all\n",
        "    best_metric = -100\n",
        "    #cv folds\n",
        "    outer_cv = KFold(n_splits=outer_k_folds, shuffle=True)\n",
        "    inner_cv = KFold(n_splits=inner_k_folds, shuffle=True)\n",
        "    #loop for each trial\n",
        "    for trial in range(1, num_trials+1):\n",
        "        #strore results per trial\n",
        "        results_dict = dict()\n",
        "        cv_results_dict = dict()\n",
        "        i = 1\n",
        "        for train, test in outer_cv.split(X):\n",
        "            #array to store scores per cv split \n",
        "            nested_scores_train = dict()\n",
        "            #to count time it took\n",
        "            start = time.time()\n",
        "            #performing inner cross validation here when looking for the best parameters\n",
        "            random_search = RandomizedSearchCV(estimator=model,\n",
        "                                param_distributions=param_grid, verbose=1, scoring=scoring,\n",
        "                                n_iter=num_iter, cv=inner_cv, n_jobs=-1)\n",
        "            #fitting training data\n",
        "            random_search.fit(X.loc[train,:], y.loc[train,:])\n",
        "            #saving results\n",
        "            run_time = time.time()-start\n",
        "            nested_scores_train['Best_Score'] = (-random_search.best_score_ if architecture == 1 else random_search.best_score_)\n",
        "            nested_scores_train['Evaluation_Score'] = (-random_search.score(X.loc[test,:], y.loc[test,:]) if architecture == 1 else random_search.score(X.loc[test,:], y.loc[test,:])) #evaluating on test data\n",
        "            nested_scores_train['Scorer'] = (scoring if architecture == 1 else str(random_search.scorer_))\n",
        "            nested_scores_train['Best_Params'] = random_search.best_params_\n",
        "            nested_scores_train['Run_Time'] = run_time\n",
        "            results_dict['Experiment_'+str(i)] = nested_scores_train\n",
        "            #store full CV results in case it is needed\n",
        "            cv_results_dict['Experiment_'+str(i)] = str(random_search.cv_results_)\n",
        "            #storing the best estimator\n",
        "            if(random_search.best_score_ > best_metric):\n",
        "                best_metric = random_search.best_score_\n",
        "                best_estimator = random_search.best_estimator_\n",
        "            #finishing fold\n",
        "            print('Outer iteration %d took %.3f s' %(i, run_time))\n",
        "            i += 1\n",
        "        #save the best scores after completing one trial\n",
        "        save_file('Personality', results_dict, trial, outer_k_folds, architecture, with_da, COLAB)\n",
        "        save_file('CV', cv_results_dict, trial, outer_k_folds, architecture, with_da, COLAB)\n",
        "    #store features importance for the best estimator (https://scikit-learn.org/stable/modules/ensemble.html)\n",
        "    #and https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-py\n",
        "    importances_arr = []\n",
        "    for clf in best_estimator.estimators_:\n",
        "        feature_importances = clf.feature_importances_\n",
        "        importances_arr.append(feature_importances)\n",
        "    df_features_importances = pd.DataFrame(data=importances_arr, columns=X.columns.values, index=y.columns.values).transpose()\n",
        "    df_features_importances.index.name = 'adjective'\n",
        "    df_features_importances = df_features_importances.reset_index()\n",
        "    #store to csv\n",
        "    save_csv(df_features_importances, architecture, with_da, COLAB)\n",
        "    #return data\n",
        "    return best_metric, best_estimator"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TWdoPdgfXeK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### Model Testing (not used to evaluate the model) #\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "'''\n",
        "Predict for some input values\n",
        "Saves a file containing some metrics regarding the predictions\n",
        "Regression Metrics: MAE, MSE and RMSE global and for each trait\n",
        "Classification Metrics: Micro \n",
        "'''\n",
        "def model_testing(estimator, X, y, architecture=1, with_da='no_da', colab=False, store_file=True):\n",
        "    #make predictions\n",
        "    predictions = estimator.predict(X)\n",
        "    #depending on the architecture, compute metrics\n",
        "    if architecture == 1:\n",
        "        #global metrics (for all traits together)\n",
        "        mae_global = mean_absolute_error(y, predictions, multioutput='uniform_average')\n",
        "        mse_global = mean_squared_error(y, predictions, multioutput='uniform_average')\n",
        "        rmse_global = np.sqrt(mse_global)\n",
        "        #individual metrics (for all traits together)\n",
        "        mae_list = mean_absolute_error(y, predictions, multioutput='raw_values').tolist()\n",
        "        mse_list = mean_squared_error(y, predictions, multioutput='raw_values').tolist()\n",
        "        rmse_list = list(map(np.sqrt, mse_list))\n",
        "        #results dictionary\n",
        "        testing_results = {\n",
        "            'MAE': mae_global,\n",
        "            'MSE': mse_global,\n",
        "            'RMSE': rmse_global,\n",
        "            'MAE_LIST': mae_list,\n",
        "            'MSE_LIST': mse_list,\n",
        "            'RMSE_LIST': rmse_list,\n",
        "            'predictions': predictions.tolist()\n",
        "        }\n",
        "    else:\n",
        "        #how many classes are wrong\n",
        "        mean_error = (sum( [sum(predictions[i] != y.values[i]) for i in np.arange(0, len(predictions))] )\n",
        "                      / (predictions.shape[0] * predictions.shape[1]))\n",
        "        #get transpose for the metrics\n",
        "        predictions_transpose = [*zip(*predictions)]\n",
        "        y_transpose = [*zip(*y.values)]\n",
        "        it_range = range(0, len(y_transpose))\n",
        "        #for micro-averaging in a multiclass setting with all labels included \n",
        "        #it will produce equal precision, recall and F\n",
        "        #https://simonhessner.de/why-are-precision-recall-and-f1-score-equal-when-using-micro-averaging-in-a-multi-class-problem/\n",
        "        f1_micro = [f1_score(y_transpose[i], predictions_transpose[i], average='micro') for i in it_range]\n",
        "        precision_micro = [precision_score(y_transpose[i], predictions_transpose[i], average='micro') for i in it_range]\n",
        "        recall_micro = [recall_score(y_transpose[i], predictions_transpose[i], average='micro') for i in it_range]\n",
        "        f1_macro = [f1_score(y_transpose[i], predictions_transpose[i], average='macro') for i in it_range]\n",
        "        precision_macro = [precision_score(y_transpose[i], predictions_transpose[i], average='macro') for i in it_range]\n",
        "        recall_macro = [recall_score(y_transpose[i], predictions_transpose[i], average='macro') for i in it_range]\n",
        "        #results dictionary\n",
        "        testing_results = {\n",
        "            'MEAN_ERROR': mean_error,\n",
        "            'F1': np.mean(f1_micro), \n",
        "            'PRECISION': np.mean(precision_micro),\n",
        "            'RECALL': np.mean(recall_micro), \n",
        "            'F1_MICRO': f1_micro,\n",
        "            'PRECISION_MICRO': precision_micro,\n",
        "            'RECALL_MICRO': recall_micro,\n",
        "            'F1_MACRO': f1_macro,\n",
        "            'PRECISION_MACRO': precision_macro,\n",
        "            'RECALL_MACRO': recall_macro,\n",
        "            'predictions': predictions.tolist()\n",
        "        }\n",
        "    #is to save a results file\n",
        "    if store_file:\n",
        "        save_file('Testing', testing_results, 0, 0, architecture, with_da, colab, testing=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XWSyGtOe3fm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### Architecture Composing #########################\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "def run_architecture(df, architecture=1):\n",
        "    #split into X and y\n",
        "    X, y = split_x_y(df, architecture, testing=False)\n",
        "    #the multi-output model\n",
        "    param_grid = tuning_dictionary()\n",
        "    multi_estimators = build_model(architecture)\n",
        "    #train and save best model\n",
        "    scoring = ('neg_root_mean_squared_error' if architecture == 1 else None)\n",
        "    best_metric, best_estimator = find_best_model(multi_estimators, X, y, param_grid, \n",
        "                                                num_trials=NUM_TRIALS, outer_k_folds=OUTER_K_FOLDS, \n",
        "                                                inner_k_folds=INNER_K_FOLDS, num_iter=RANDOM_ITERATIONS,\n",
        "                                                scoring=scoring,\n",
        "                                                architecture=architecture, with_da=DA_DESC)\n",
        "    print('Best overall %s for architecture %d and %s is %.4f' %(('RMSE' if architecture == 1 else 'AUC'), architecture, DA_DESC, best_metric))\n",
        "    #testing the best model\n",
        "    X, y = split_x_y(df, architecture, testing=True)\n",
        "    model_testing(best_estimator, X, y, architecture=architecture, with_da=DA_DESC, colab=COLAB, store_file=True)\n",
        "    #store best model\n",
        "    save_best_estimator(best_estimator, architecture, DA_DESC, COLAB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jqNx9CWH-eIv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''\n",
        "#####################################################\n",
        "#### Main Execution #################################\n",
        "#####################################################\n",
        "'''\n",
        "\n",
        "#Global Vars\n",
        "COLAB = True\n",
        "ARCHITECTURE = 2\n",
        "WITH_DA = True\n",
        "DA_DESC = ('WithDa' if WITH_DA else 'NoDa')\n",
        "NUM_TRIALS = 2\n",
        "OUTER_K_FOLDS = 3\n",
        "INNER_K_FOLDS = 4\n",
        "RANDOM_ITERATIONS = 200\n",
        "\n",
        "#Read dataset\n",
        "df = read_dataset(WITH_DA, COLAB)\n",
        "run_architecture(df, architecture=ARCHITECTURE)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}